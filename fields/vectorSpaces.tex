\documentclass{ximera}

\input{../preamble.tex}

\author{Bart Snapp}

\title{Vector spaces}

\begin{document}
\begin{abstract}
  We review vector spaces.
\end{abstract}
\maketitle



\begin{definition}\index{K-vector space@$K$-vector space}\index{vector space@$K$-vector space}
  A \textbf{$\boldsymbol{K}$-vector space} is an Abelian group $(V,+)$
  along with a field $K$ such that we may multiply group elements by
  field elements, meaning that there is a binary operation $-\cdot-:
  K\times V \to V$ such that if $\nu,\mu\in V$ and $a,b,\in K$ we
  have:
\begin{description}
\item[Compatibility with scalars] $(ab)\cdot \nu = a\cdot (b\cdot \nu)$.
\item[Vectors distribute over scalars] $(a+b)\cdot \nu =
  a\cdot\nu + b\cdot \nu$.
\item[Scalars distribute over vectors] $a\cdot (\nu+\mu) =
  a\cdot \nu + a\cdot \mu$.
\item[Identity is respected] $1_K\cdot \nu = \nu$.
\end{description}
In this case, elements of the group $V$ are called \dfn{vectors} and
elements of the field $K$ are called \dfn{scalars}.
\end{definition}

\begin{exercise}
  Let $V$ be a $K$-vector space. If $\nu\in V$, prove that
  $0_K\cdot \nu = \vec{0}$.
\end{exercise}


\begin{example}[Euclidean space]
  Perhaps the most obvious vector space would be $\R^3$. This is an
  $\R$-vector space. It is also a $\Q$-vector space.
\end{example}



\begin{example}[Complex numbers]
  Recall $\C = \{a+bi:a,b\in\R\}$. The set $\C$ is an $\C$-vector
  space, an $\R$-vector space, and a $\Q$-vector space.
\end{example}


\begin{example}[Polynomials]
  Let $K[x]$ be the set of all formal sums
  \[
  c_nx^n + c_{n-1}x^{n-1} + \dots + c_1 x + c_0
  \]
  where $n$ is a nonnegative integer and each $c_i \in K$. In this
  case, $K[x]$ is a $K$-vector space.
\end{example}




\begin{definition}
  Let $V$ be a $K$-vector space. A subset $W\subset V$ is a
  \textbf{$\boldsymbol K$-vector} \dfn{subspace} of $V$ if $W\subgp V$ and $W$
  is also a $K$-vector space.
\end{definition}

\begin{lemma}[Subspace criterion]\index{subspace criterion}
  Let $V$ be a $K$-vector space. $W\subset K$ is a subspace of $V$ if
  and only if
  \begin{enumerate}
  \item $W\ne \emptyset$.
  \item $W$ is closed under multiplication by scalars.
  \item $W$ is closed under vector addition.
  \end{enumerate}
  \begin{sketch}
    Check the definition of a vector space.
  \end{sketch}
\end{lemma}


\begin{exercise}
  Let $V$ be a $K$-vector space with subspaces $U$ and $W$. Prove that
  $U\cap W$ is a $K$-vector subspace of $V$.
\end{exercise}

\begin{definition}
  Let $V$ and $W$ be $K$-vector spaces. A \dfn{linear transformation}
  for $\nu,\mu\in V$ and $s\in K$ is a function $T:V\to W$ such that
    \begin{enumerate}
    \item $T(\nu+\mu) = T(\nu)+T(\mu)$.
    \item $T(s \nu) = sT(\nu)$.
    \end{enumerate}
\end{definition}

\begin{remark}
  We proved that every linear transformation is a matrix in
  Lemma~\ref{L:MT}.
\end{remark}

\begin{definition}\index{isomorphic!vector spaces}
  Two $K$-vector spaces $V$ and $W$ are said to be \textbf{isomorphic
    vector spaces} if there exists a bijective linear transformation
  $T:V\to W$. In this case we write, $V\iso W$.
\end{definition}

\begin{exercise}\index{kernel!linear transformation}
  Let $V$ and $W$ be $K$-vector spaces and let $T:V\to W$ be a linear
  transformation. Define the \textbf{kernel} of $T$ as follows:
  \[
  \ker(T) = \{\nu\in V: T(\nu) = 0\}\subset V.
  \]
  Prove that $\ker(T)$ is a $K$-vector subspace of $V$.
\end{exercise}

\begin{exercise}\index{image!linear transformation}
  Let $V$ and $W$ be $K$-vector spaces and let $T:V\to W$ be a linear
  transformation. Define the \textbf{image} of $T$ as follows:
  \[
  \im(T) = \{T(\nu): \nu\in V\}\subset W.
  \]
  Prove that $\im(T)$ is a $K$-vector subspace of $W$.
\end{exercise}







\begin{definition}
  Given a set of vectors $S$, in a $K$-vector space, $V$, the
  \dfn{span} of the vectors in $S$ is
  \[
  \Span(S) = \left\{\sum_{i=1}^n a_i\sigma_i:\text{$n\in \N$,
    $\sigma_i\in S$, and $a_i\in K$}\right\}.
  \]
  If $\Span(S) = V$, then we say $S$ is a \dfn{spanning set}.
\end{definition}





\begin{definition}
  Given a $K$-vector space $V$, a finite set of vectors
  \[
  \{\lambda_1,\dots,\lambda_n\}
  \]
  is said to be \dfn{linearly independent} if
  \[
  a_1\lambda_1 + a_2\lambda_2 +\cdots + a_n\lambda_n = 0\quad \Rightarrow \quad a_1= \cdots =a_n = 0.
  \]
  A finite set of vectors is set to be \dfn{linearly dependent} if
  they are not linearly independent.
\end{definition}



\begin{exercise}
  Let $V$ be a $K$-vector space. Let $S= \{\sigma_1,\sigma_2,
  \dots,\sigma_n\}$. Prove that if $\tau\notin\Span(S)$, that $\tau$
  and $\sigma_i$ are linearly independent.
\end{exercise}


%% \begin{exercise}
%%   Let $V$ be a $K$-vector space. Let $L= \{\lambda_1,\lambda_2,
%%   \dots,\lambda_n\}$. Prove that if , that $\tau$
%%   and $\sigma_i$ are linearly independent.
%% \end{exercise}



\begin{theorem}[Bases equivalences]
  Define a partial ordering on sets where $S \preceq T$ if $S\subset
  T$. Let $B= \{\beta_1,\dots,\beta_n\}$ be a finite set of vectors in
  a $K$-vector space $V$. The following are equivalent:
  \begin{enumerate}
  \item $B = \min_{\preceq}\{S\subset V:\Span(S) = V\}$.
  \item $B$ is a linearly independent spanning set of vectors.
  \item $B = \max_{\preceq}\{S\subset V:\text{$S$ is a linearly independent set of vectors}\}$.
  \end{enumerate}
  Any finite set of vectors $B \subset V$ satisfying any of the
  equivalent conditions above is called a \dfn{basis} for $V$.
  \begin{proof}
    We will prove this in a ``circle.''


    $(\mathrm a)\Rightarrow(\mathrm b)$ We will assume that
    \[
    B = \min_{\preceq}\{S\subset V:\Span(S) = V\}.
    \]
    We must show that $B$ is a linearly independent spanning set of
    vectors.  If the set $B$ is minimal with respect to spanning $V$,
    no subset of $B$ will span $V$. In particular, if
    \[
    B = \{\beta_1,\beta_2,\dots,\beta_n\},
    \]
    then
    \[
    \beta_n \notin \Span(\beta_1,\dots,\beta_{n-1}).
    \]
    This means $\beta_n$ and $\beta_i$ are linearly independent for $i
    = 1,\dots, n-1$. Repeating this process with each $\beta_i$, we
    see that $B$ is a spanning set of linearly independent vectors.


    $(\mathrm b)\Rightarrow(\mathrm c)$ We will assume that $B$ is a
    linearly independent spanning set of vectors. We must show that
    \[
    B = \max_{\preceq}\{S\subset V:\text{$S$ is a linearly independent set of vectors}\}.
    \]
    If $B$ is a set of linearly independent vectors that span $V$,
    then no vector can be added to this list while maintaining linear
    independence. Hence $B$ is a maximal linearly independent set.
    

    $(\mathrm c)\Rightarrow(\mathrm a)$ We will assume that
    \[
    B = \max_{\preceq}\{S\subset V:\text{$S$ is a linearly independent set of vectors}\}.
    \]
    We must show that
    \[
    B = \min_{\preceq}\{S\subset V:\Span(S) = V\}.
    \]
    If $B$ is a maximal linearly independent set, no vectors can be
    added while maintaining linear independence, thus $\Span(B) =
    V$. If any element were removed from $B$, it would no longer span
    $V$, hence $B$ is a minimal spanning set.
  \end{proof}
\end{theorem}



\begin{theorem}[Bases have the same order]
  Let $V$ be a $K$-vector space with bases
  \begin{align*}
    B &= \{\beta_1,\beta_2,\dots, \beta_n\}, \\
    C &= \{\gamma_1,\gamma_2, \dots, \gamma_m\}.
  \end{align*}
  In this case, $n = m$.
  \begin{proof}
    Let $B= \{\beta_1,\beta_2,\dots, \beta_n\}$ be a basis for $V$.
    Suppose you want to construct another basis, $C$.  Let $\gamma_1$
    be the first vector you choose to be in $C$. Since $\Span(B) = V$,
    we may write
    \begin{align}
      \gamma_1 &= c_1\beta_1 + c_2\beta_2 + \dots + c_n\beta_n \tag{$\bigstar$}\\
      \beta_1 &= c_1^{-1}(\gamma_1 -  c_2\beta_2 - \dots - c_n\beta_n.\notag
    \end{align}
    Note, WLOG $c_1 \ne 0$, as if it was zero, we could renumber our
    basis vectors.  Letting $B_1 = \{\gamma_1, \beta_2,\dots,
    \beta_n\}$, this means that $\beta_1\in \Span(B_1)$. We claim that
    $B_1$ is a set of linearly independent vectors. Suppose that for
    $a_i\in K$
    \[
    a_1 \gamma_1 + a_2 \beta_2 + \dots + a_n \beta_n = 0.
    \]
    Using $(\bigstar)$ to substitute, write
    \begin{align*}
      a_1(c_1\beta_1 + c_2\beta_2 + \dots + c_n\beta_n) + a_2 \beta_2 + \dots + a_n \beta_n &= 0\\
      (a_1c_1) \beta_1 + (a_2+a_1c_2)\beta_2 + \dots + (a_n+a_1c_n)\beta_n &= 0.
    \end{align*}
    Since $B$ is a set of linearly independent vectors, we conclude
    \begin{align*}
      (a_1c_1) &=0\\
      (a_2+a_1c_2) &=0\\
      &\vdots \\      
      (a_n+a_1c_n) &=0.
    \end{align*}
    Since $c_1\ne 0$, we see that $a_i = 0$, and hence $B_1$ is a set
    of linearly independent vectors. Inductively repeating this
    process for $\gamma_2\notin \Span(\gamma_1)$, and then for
    $\gamma_3 \notin \Span(\gamma_1,\gamma_2)$, we construct sets of
    linearly independent spanning vectors
    \begin{align*}
      B &= \{\beta_1,\beta_2,\dots,\beta_n\},\\
      B_1 &= \{\gamma_1,\beta_2,\dots,\beta_n\},\\
      B_2 &= \{\gamma_1,\gamma_2,\dots,\beta_n\},\\
      &\vdots \\
      C = B_n &= \{\gamma_1,\gamma_2,\dots,\gamma_n\}.
    \end{align*}
    Since $C$ is a set spanning linearly independent vectors, we
    cannot add another linearly independent vector, thus $|C| = |B|$.
  \end{proof}
\end{theorem}


Here is a picture that attempts to convey the situation:
\[
\begin{tikzpicture}
  \draw[white,thin,shading = axis, top color = white, bottom color = gray] (-2,2) -- (0,0)-- (2,2) -- (-2,2) -- cycle;
  \draw[white,thin,shading = axis, bottom color = white, top color = gray] (-2,-2) -- (0,0)-- (2,-2) -- (-2,-2) -- cycle;
  \filldraw (0,0) circle (3pt);
  \node[right] at (0,0) {$B$ is linearly independent and spans};
  \node at (0,1.5) {$\scriptstyle \Span(B) = V$};
  \node at (0,-1.5) {\tiny $B$  is linearly independent};
  \draw[->,ultra thick] (-3,-2) -- (-3,2);
  \node[left] at (-3,-1.5) {\text{\tiny $|B|$ is smaller}};
  \node[left] at (-3,1.5) {\text{\tiny $|B|$ is larger}};
\end{tikzpicture}
\]
The cone above $B$ are all sets of vectors that span $V$. The cone
below $B$ are all sets of vectors that are linearly independent.



\begin{definition}
  Let $B$ be a basis for a $K$-vector space $V$. The \dfn{vector space
    dimension} of $V$ is the order of $B$,
  \[
  \dim_K(V) = |B|.
  \]
  If a vector space does not have a finite basis, then we say it is
  \dfn{infinite dimensional}.
\end{definition}


\begin{example}[Euclidean space]
  The $\R$-vector space $\R^3$ has dimension $3$ since it has basis vectors
  \[
  \begin{bmatrix}
    1\\
    0\\
    0
  \end{bmatrix},\quad
  \begin{bmatrix}
    0\\
    1\\
    0
  \end{bmatrix},\quad
   \begin{bmatrix}
    0\\
    0\\
    1
  \end{bmatrix}.
  \]
\end{example}


\begin{example}[Complex numbers]
   Recall $\C = \{a+bi:a,b\in\R\}$. The complex numbers are a vector
   space over several different fields. Here we have
  \[
  \dim_{\C}(\C) = 1, \quad \dim_{\R}(\C) = 2, \quad \dim_{\Q}(\C) =\infty.
  \]
\end{example}

\begin{exercise}
  Find a basis for $\C$ as a $\C$-vector space. Find a basis for $\C$
  as an $\R$-vector space. In each case, prove your answer is correct.
\end{exercise}



\begin{example}[Polynomials of fixed degree]
  Let $\mathcal{P}_3\subset \Q[x]$ be the polynomials up to and including degree
  $3$. In this case $\dim_{\Q}(\Q[x]) = 4$.
\end{example}

\begin{exercise}
  Find a basis for $\mathcal{P}_3\subset \Q[x]$, the polynomials up to and
  including degree $3$, as a $\Q$-vector space. Prove your answer is
  correct.
\end{exercise}


\begin{theorem}[Bases and unique representation]
  Let $V$ be a $K$-vector space and $B = \{\beta_1,\dots,\beta_n$ be a
  basis for $V$. If
  \begin{align*}
  \alpha = a_1\beta_1 + a_2\beta_2 + \dots + a_n\beta_n\\
  \alpha = c_1\beta_1 + c_2\beta_2 + \dots + c_n\beta_n,
  \end{align*}
  where $a_i, c_i\in K$, then $a_i = c_i$ for $i =1,\dots, n$.
  \begin{sketch}
    Subtract the two equations above, combine like terms, and use the
    fact that a basis is a linearly independent set of vectors.
  \end{sketch}
\end{theorem}


\begin{corollary}
  If $V$ is an $n$ dimensional $K$-vector space, then $V\iso K^n$.
\end{corollary}

\begin{exercise}
  Let $V$ be a $K$-vector space of dimension $4$. Let $U$ and $W$ be
  subspaces of $V$. If $\dim_K(U) = 3$ and $\dim_K(V) =3$, what could
  be the value of $\dim_K(W\cap V)$?
\end{exercise}

\begin{exercise}
  Let $V$ be a $\Z_5$-vector space of dimension $3$. How many elements
  are in $V$?
\end{exercise}



\begin{theorem}[Quotient vector spaces]
  Given a $K$-vector space $V$ and a subspace $W\subset V$, the set of
  left cosets of $W$ form a $K$-vector space under coset
  addition. This $K$-vector space is denoted by $V/W$ and is
  pronounced ``$V$ modulo $W$.''
  \begin{proof}
    First note that by Theorem\ref{T:quotient}, $V/W$ is an Abelian
    group under coset addition. Now we must show that multiplication
    by field elements is \index{well-defined}well-defined and that
    $V/W$ is a $K$-vector space. Suppose that
    \[
    \nu + W = \mu + W.
    \]
    We must show that if $a\in K$,
    \[
    a(\nu + W) = a(\mu + W).
    \]
    Since $aW = W$, this is true. All other conditions for $V/W$ to be
    a vector space are inherited from the fact that $V$ is a $K$-vector
    space.
  \end{proof}
\end{theorem}

\begin{corollary}[Dimensions and quotients]
  Let $V$ be a $K$-vector space and $W\subset V$ be a subspace. In
  this case,
  \[
  \dim_K(V) = \dim_K(W) + \dim_K(V/W)
  \]
\end{corollary}

%% \begin{example}[$\R^n$]
%% \end{example}


\begin{corollary}[Dimension and linear transformations]
  Let $V$ and $W$ be a $K$-vector spaces and let $T:V\to W$ be a
  linear transformation. In this case
  \[
  \dim_K(V) = \dim_K(\ker(T)) + \dim_K(\im(T)).
  \]
  \begin{sketch}
    Use Noether's isomorphism theorem, Theorem~\ref{T:NI}.
  \end{sketch}
\end{corollary}


\end{document}
