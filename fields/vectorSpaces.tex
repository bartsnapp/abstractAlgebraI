\documentclass{ximera}

\input{../preamble.tex}

\author{Bart Snapp}

\title{Vector spaces}

\begin{document}
\begin{abstract}
  We review vector spaces.
\end{abstract}
\maketitle



\begin{definition}\index{K-vector space@$K$-vector space}\index{vector space@$K$-vector space}
  A \textbf{$\boldsymbol{K}$-vector space} is an Abelian group $(V,+)$
  along with a field $K$, whose elements are called \dfn{scalars},
  such that we may multiply group elements by field elements, meaning
  that there is a binary operation $-\cdot-: K\times V \to V$ such that
  if $\nu,\mu\in V$ and $a,b,\in K$ we have:
\begin{description}
\item[Compatibility with scalars] $(ab)\cdot \nu = a\cdot (b\cdot \nu)$.
\item[Vectors distribute over scalars] $(a+b)\cdot \nu =
  a\cdot\nu + b\cdot \nu$.
\item[Scalars distribute over vectors] $a\cdot (\nu+\mu) =
  a\cdot \nu + a\cdot \mu$.
\item[Identity is respected] $1_K\cdot \nu = \nu$.
\end{description}
In this case, elements of the group $V$ are called \dfn{vectors} and
elements of the field $K$ are called \dfn{scalars}.
\end{definition}

\begin{exercise}
  Let $V$ be a $K$-vector space. If $\nu\in V$, prove that
  $0_K\cdot \nu = \vec{0}$.
\end{exercise}


\begin{example}[Euclidean space]
\end{example}



\begin{example}[Complex numbers]
  The set $\C$ is an $\R$-vector space.
\end{example}


\begin{example}[Polynomials of fixed degree]
  A polynomial 
\end{example}




\begin{definition}
  Let $V$ be a $K$-vector space. A subset $W\subset V$ is a
  \textbf{$\boldsymbol K$-vector} \dfn{subspace} of $V$ if $W\subgp V$ and $W$
  is also a $K$-vector space.
\end{definition}

\begin{lemma}[Subspace criterion]\index{subspace criterion}
  Let $V$ be a $K$-vector space. $W\subset K$ is a subspace of $V$ if
  and only if
  \begin{enumerate}
  \item $W\ne \emptyset$.
  \item $W$ is closed under multiplication by scalars.
  \item $W$ is closed under vector addition.
  \end{enumerate}
  \begin{sketch}
    Check the definition of a vector space.
  \end{sketch}
\end{lemma}



\begin{definition}
  Given a set of vectors $S$, in a $K$-vector space, $V$, the
  \dfn{span} of these vectors is
  \[
  \Span(S) = \{a\sigma+b\tau:\text{$s,t\in S$ and $a,b\in K$}\}.
  \]
\end{definition}


\begin{definition}
  Given a $K$-vector space $V$, a finite set of vectors
  \[
  \{\lambda_1,\dots,\lambda_n\}
  \]
  is said to be \dfn{linearly independent} if
  \[
  a_1\lambda_1 + a_2\lambda_2 +\cdots + a_n\lambda_n = 0\quad \Rightarrow \quad a_1= \cdots =a_n = 0.
  \]
  A finite set of vectors is set to be \dfn{linearly dependent} if
  they are not linearly independent.
\end{definition}

\begin{theorem}[Bases equivalences]
  Define an ordering on sets where $S \preceq T$ if $|S|\le |T|$. Let
  $B= \{\beta_1,\dots,\beta_n\}$ be a finite set of vectors in a
  $K$-vector space $V$. The following are equivalent:
  \begin{enumerate}
  \item $B = \min_{\preceq}\{S\subset V:\Span(S) = V\}$.
  \item $B$ is a linearly independent spanning set of vectors.
  \item $B = \max_{\preceq}\{S\subset V:\text{$S$ is a linearly independent set of vectors}\}$.
  \end{enumerate}
  Any finite set of vectors $B \subset V$ satisfying any of the
  equivalent conditions above is called a \dfn{basis} for $V$.
  \begin{proof}
    We will prove this in a ``circle.''

    
    $(\mathrm a)\Rightarrow(\mathrm b)$ We will assume that
    \[
    B = \min_{\preceq}\{S\subset V:\Span(S) = V\}.
    \]
    We must show that $B$ is a linearly independent spanning set of
    vectors. Seeking a contradiction, suppose that $B$ is not a
    linearly independent set of vectors. Then there exist $a_i\in K$
    such that
    \[
    a_1\beta_1 + a_2\beta_2 + \dots + a_n\beta_n = 0
    \]
    where not all the $a_i$ are nonzero. WLOG, we may assume that $a_n
    \ne 0$. Then we may write
    \[
    a_1\beta_1 + a_2\beta_2 + \dots + a_{n-1}\beta_{n-1}= -a_n \beta_n
    \]
    and we see that $B' = \{\beta_1,\dots,\beta_{n-1}\}$ spans
    $V$. But $B'\preceq B$, a contradiction. Hence $B$ is a linearly
    independent spanning set of vectors.

    

    $(\mathrm b)\Rightarrow(\mathrm c)$ We will assume that $B$ is a
    linearly independent spanning set of vectors. We must show that
    \[
    B = \max_{\preceq}\{S\subset V:\text{$S$ is a linearly independent set of vectors}\}.
    \]
    Suppose you want to construct another set $C$ of linearly
    independent vectors. Suppose that $\gamma_1$ is the first vector
    you choose to be in $C$. Since $\Span(B) = V$, we may write
    \begin{align}
      \gamma_1 &= c_1\beta_1 + c_2\beta_2 + \dots + c_n\beta_n \tag{$\bigstar$}\\
      \beta_1 &= c_1^{-1}(\gamma_1 -  c_2\beta_2 - \dots - c_n\beta_n.\notag
    \end{align}
    Note, WLOG $c_1 \ne 0$, as if it was zero, we could renumber our
    basis vectors.  Letting $B_1 = \{\gamma_1, \beta_2,\dots,
    \beta_n\}$, this means that $\beta_1\in \Span(B_1)$. We claim that
    $B_1$ is a set of linearly independent vectors. Suppose that for
    $a_i\in K$
    \[
    a_1 \gamma_1 + a_2 \beta_2 + \dots + a_n \beta_n = 0.
    \]
    Using $(\bigstar)$ to substitute, write
    \begin{align*}
      a_1(c_1\beta_1 + c_2\beta_2 + \dots + c_n\beta_n) + a_2 \beta_2 + \dots + a_n \beta_n = 0\\
      (a_1c_1) \beta_1 + (a_2+a_1c_2)\beta_2 + \dots + (a_n+a_1c_n)\beta_n = 0.
    \end{align*}
    Since $B$ is a set of linearly independent vectors, we conclude
    \begin{align*}
      (a_1c_1) &=0\\
      (a_2+a_1c_2) &=0\\
      &\vdots \\      
      (a_n+a_1c_n) &=0.
    \end{align*}
    Since $c_1\ne 0$, we see that $a_i = 0$, and hence $B_1$ is a set
    of linearly independent vectors. Inductively repeating this
    process for $\gamma_2\notin \Span(\gamma_1)$, and then for
    $\gamma_3 \notin \Span(\gamma_1,\gamma_2)$, we construct sets of
    linearly independent spanning vectors
    \begin{align*}
      B &= \{\beta_1,\beta_2,\dots,\beta_n\},\\
      B_1 &= \{\gamma_1,\beta_2,\dots,\beta_n\},\\
      B_2 &= \{\gamma_1,\gamma_2,\dots,\beta_n\},\\
      &\vdots \\
      C = B_n &= \{\gamma_1,\gamma_2,\dots,\gamma_n\}.\\
    \end{align*}
    Since $C$ is a set spanning linearly independent vectors, we
    cannot add another linearly independent vector. Since $|C| = |B|$,
    \[
    B = \max_{\preceq}\{S\subset V:\text{$S$ is a linearly independent set of vectors}\}.
    \]
    
    $(\mathrm c)\Rightarrow(\mathrm a)$
    
  \end{proof}
\end{theorem}


Here is a picture that attempts to convey the situation:
\[
\begin{tikzpicture}
  \draw[white,thin,shading = axis, top color = white, bottom color = gray] (-2,2) -- (0,0)-- (2,2) -- (-2,2) -- cycle;
  \draw[white,thin,shading = axis, bottom color = white, top color = gray] (-2,-2) -- (0,0)-- (2,-2) -- (-2,-2) -- cycle;
  \filldraw (0,0) circle (3pt);
  \node[right] at (0,0) {$B$ is linearly independent and spans};
  \node at (0,1.5) {$\scriptstyle \Span(B) = V$};
  \node at (0,-1.5) {\tiny $B$  is linearly independent};
\end{tikzpicture}
\]
The cone above $B$ are all sets of vectors that span $V$. The cone
below $B$ are all sets of vectors that are linearly independent.



\begin{corollary}[Bases have the same order]
  Let $V$ be a $K$-vector space. If $B$ and $B'$ are two bases for $V$
  over $K$, then $|B| = |B'|$
\end{corollary}


\begin{definition}
  Let $B$ be a basis for a $K$-vector space $V$. The \dfn{vector-space
    dimension} of $V$ is the order of $B$,
  \[
  \dim_K(V) = |B|.
  \]
  If a vector space does not have a finite basis, then we say it is
  \dfn{infinite dimensional}.
\end{definition}


\begin{example}[Euclidean space]
\end{example}


\begin{example}[Complex numbers]
\end{example}



\begin{example}[Polynomials of fixed degree]
\end{example}




\begin{theorem}[Quotient vector spaces]
  Given a $K$-vector space $V$ and a subspace $W\subset V$, the set of
  left cosets of $W$ form a $K$-vector space under coset
  addition. This $K$-vector space is denoted by $V/W$ and is
  pronounced ``$V$ modulo $W$.''
  \begin{proof}
    First note that by Theorem\ref{T:quotient}, $V/W$ is an Abelian
    group under coset addition. Now we must show that multiplication
    by field elements is \index{well-defined}well-defined and that
    $V/W$ is a $K$-vector space. Suppose that
    \[
    \nu + W = \mu + W.
    \]
    We must show that if $a\in K$,
    \[
    a\nu + W = a\mu + W.
    \]
    Since $aW = W$, this is true. All other conditions for $V/W$ to be
    a vector space are inherited from the fact that $V$ is a $K$-vector
    space.
  \end{proof}
\end{theorem}

\end{document}
